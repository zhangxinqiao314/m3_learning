{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "KB7KSC5r1Qar",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tutorial: Cycle-Consistent Spatial Transforming Autoencoders\n",
    "\n",
    "By Shuyu Qin, Joshua C. Agar\n",
    "\n",
    "Department of Mechanical Engineering and Mechanics\n",
    "Drexel University\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aIPhuxCqx049",
    "outputId": "a8b871fe-2b6e-4992-d1cb-81ec0300db77",
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install m3_learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-GbJmyO_1itx",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Configuration\n",
    "\n",
    "### Imports packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "29lMNJV9xe7w",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "from skimage.filters import sobel\n",
    "import tarfile\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "8R1uHT1P1sPq",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Downloads Files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8V3z9ZZFyUiu",
    "outputId": "28a3773f-3eb6-40da-b859-30056d8e1767",
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "!wget - O cards.tar.xz https: // github.com/m3-learning/m3_learning/blob/main/m3_learning/Tutorials/Cycle_Consistent_Spatial_Transformer_Autoencoder/data/cards.tar.xz?raw = true\n",
    "!wget - O 11.12_unsupervised_learn_label_epoch_17957_coef_0_trainloss_0.0008.pkl https: // github.com/m3-learning/Unsupervised-rotation-detection-and -label-learning/blob/main/11.12_unsupervised_learn_label_epoch_17957_coef_0_trainloss_0.0008.pkl?raw = true\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "XGnsP5VK14Wa",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Extracts Files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aFQrlSJLyiMm",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "file = tarfile.open('./cards.tar.xz')\n",
    "file.extractall('./')\n",
    "file.close()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "-3AkKueg2IWu",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "### Conversion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D4At_FTxxe7x",
    "outputId": "011c308f-7b07-45b4-995d-7136faee87ee",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Converts images to grayscale\n",
    "card1 = cv2.imread(\"cards/card1.JPG\", cv2.IMREAD_GRAYSCALE)\n",
    "card2 = cv2.imread(\"cards/card2.JPG\", cv2.IMREAD_GRAYSCALE)\n",
    "card3 = cv2.imread(\"cards/card3.JPG\", cv2.IMREAD_GRAYSCALE)\n",
    "card4 = cv2.imread(\"cards/card4.JPG\", cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Normalizes the image based on the max value\n",
    "card1 = torch.tensor(1 - card1 / card1.max())\n",
    "card2 = torch.tensor(1 - card2 / card2.max())\n",
    "card3 = torch.tensor(1 - card3 / card3.max())\n",
    "card4 = torch.tensor(1 - card4 / card4.max())\n",
    "\n",
    "# Resizes the image\n",
    "card_small_1 = F.interpolate(card1.unsqueeze(0).unsqueeze(\n",
    "    1), size=(48, 48)).clone().type(torch.float).cpu().numpy()\n",
    "card_small_2 = F.interpolate(card2.unsqueeze(0).unsqueeze(\n",
    "    1), size=(48, 48)).clone().type(torch.float).cpu().numpy()\n",
    "card_small_3 = F.interpolate(card3.unsqueeze(0).unsqueeze(\n",
    "    1), size=(48, 48)).clone().type(torch.float).cpu().numpy()\n",
    "card_small_4 = F.interpolate(card4.unsqueeze(0).unsqueeze(\n",
    "    1), size=(48, 48)).clone().type(torch.float).cpu().numpy()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "diJ86fln2W69",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Raw Data Visulalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "id": "MzZTavM5xe7y",
    "outputId": "62889007-1bcc-4f98-80d4-76e1cf1fb7ef",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.imshow(card_small_1.squeeze())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "tQU-TTGs2jWy",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Edge detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vBgdUW0Exe7y",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Sobel edge detection\n",
    "card_edge_1 = sobel(card_small_1.squeeze())\n",
    "card_edge_2 = sobel(card_small_2.squeeze())\n",
    "card_edge_3 = sobel(card_small_3.squeeze())\n",
    "card_edge_4 = sobel(card_small_4.squeeze())\n",
    "\n",
    "# Convert to a tensor\n",
    "card_edge_1 = torch.tensor(\n",
    "    card_edge_1, dtype=torch.float).unsqueeze(0).unsqueeze(1)\n",
    "card_edge_2 = torch.tensor(\n",
    "    card_edge_2, dtype=torch.float).unsqueeze(0).unsqueeze(1)\n",
    "card_edge_3 = torch.tensor(\n",
    "    card_edge_3, dtype=torch.float).unsqueeze(0).unsqueeze(1)\n",
    "card_edge_4 = torch.tensor(\n",
    "    card_edge_4, dtype=torch.float).unsqueeze(0).unsqueeze(1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "mubTbD6Y2xEH",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Data Generation\n",
    "\n",
    "This will generate a bunch of images that are rotated from the initial position\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jYROdDpxxe7y",
    "outputId": "e0df3053-3ec5-41b4-bb9f-3776825fc481",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# sets the range and the number of steps to generate images\n",
    "angle = torch.linspace(0, 2*np.pi, 2000)\n",
    "\n",
    "# predefines a list\n",
    "input_data_set_1 = []\n",
    "input_data_set_2 = []\n",
    "input_data_set_3 = []\n",
    "input_data_set_4 = []\n",
    "\n",
    "for q in angle:\n",
    "    # defines the theta matrix for an affine transformation\n",
    "    theta = torch.tensor([\n",
    "        [torch.cos(q), torch.sin(q), 0],\n",
    "        [-torch.sin(q), torch.cos(q), 0]\n",
    "    ], dtype=torch.float)\n",
    "\n",
    "    # calculates the grid for the affine transformation\n",
    "    grid = F.affine_grid(theta.unsqueeze(0), card_edge_1.size())\n",
    "\n",
    "    # applies the affine transformation to the image\n",
    "    output = F.grid_sample(card_edge_1, grid)\n",
    "    # adds the image to the dataset\n",
    "    input_data_set_1.append(output)\n",
    "\n",
    "    output = F.grid_sample(card_edge_2, grid)\n",
    "    input_data_set_2.append(output)\n",
    "\n",
    "    output = F.grid_sample(card_edge_3, grid)\n",
    "    input_data_set_3.append(output)\n",
    "\n",
    "    output = F.grid_sample(card_edge_4, grid)\n",
    "    input_data_set_4.append(output)\n",
    "\n",
    "# combines all the data together\n",
    "input_set_1 = torch.stack(input_data_set_1).squeeze(1)\n",
    "input_set_2 = torch.stack(input_data_set_2).squeeze(1)\n",
    "input_set_3 = torch.stack(input_data_set_3).squeeze(1)\n",
    "input_set_4 = torch.stack(input_data_set_4).squeeze(1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ob48pgqA3cWN",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Visualize example rotated images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "id": "Sk_uczFjxe7y",
    "outputId": "bfa01918-0748-40ad-bedc-7a125f05371d",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.imshow(card_edge_1.squeeze())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "RkBjGr7B3lTY",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Formulate a single dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZDZZp451xe7z",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "input_set = torch.cat(\n",
    "    (input_set_1, input_set_2, input_set_3, input_set_4), axis=0)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "jDjcsxgV3vIT",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visualize example images in training data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ocRbDC50xe7z",
    "outputId": "2b5aa34c-7ad8-43be-bd65-f3663605da48",
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(2000):\n",
    "    if i % 200 == 0:\n",
    "        plt.figure()\n",
    "        plt.imshow(input_set_4[i].squeeze())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "TedTGO8S35AE",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Builds the Neural Network\n",
    "\n",
    "### Convolutional Block\n",
    "\n",
    "This is a standard convolutional block with ReLu. Each block has 4 layers. There is a layer normalization, and ResNet-like message passing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eXCJoAUexe7z",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class conv_block(nn.Module):\n",
    "    def __init__(self, t_size, n_step):\n",
    "        super(conv_block, self).__init__()\n",
    "        self.cov1d_1 = nn.Conv2d(\n",
    "            t_size, t_size, 3, stride=1, padding=1, padding_mode='zeros')\n",
    "        self.cov1d_2 = nn.Conv2d(\n",
    "            t_size, t_size, 3, stride=1, padding=1, padding_mode='zeros')\n",
    "        self.cov1d_3 = nn.Conv2d(\n",
    "            t_size, t_size, 3, stride=1, padding=1, padding_mode='zeros')\n",
    "        self.norm_3 = nn.LayerNorm(n_step)\n",
    "        self.relu_1 = nn.ReLU()\n",
    "        self.relu_2 = nn.ReLU()\n",
    "        self.relu_3 = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_input = x\n",
    "        out = self.cov1d_1(x)\n",
    "        out = self.relu_1(out)\n",
    "        out = self.cov1d_2(out)\n",
    "        out = self.relu_2(out)\n",
    "        out = self.cov1d_3(out)\n",
    "        out = self.norm_3(out)\n",
    "        out = self.relu_3(out)\n",
    "        out = out.add(x_input)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "R0SqTPPh4Tiy",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Idenity Block\n",
    "\n",
    "This is a single convolutional layer with a layer norm and ReLu activation function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U86eWxQNxe7z",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class identity_block(nn.Module):\n",
    "    def __init__(self, t_size, n_step):\n",
    "        super(identity_block, self).__init__()\n",
    "        self.cov1d_1 = nn.Conv2d(\n",
    "            t_size, t_size, 3, stride=1, padding=1, padding_mode='zeros')\n",
    "        self.norm_1 = nn.LayerNorm(n_step)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_input = x\n",
    "        out = self.cov1d_1(x)\n",
    "        out = self.norm_1(out)\n",
    "        out = self.relu(out)\n",
    "        return out\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "QiudNnwF4n2j",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Encoder construction\n",
    "\n",
    "This constructs the encoder using the convolutional and identity blocks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u6MSmcRCxe7z",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, original_step_size, pool_list, embedding_size, conv_size):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        # list of blocks\n",
    "        blocks = []\n",
    "\n",
    "        # defines the orignal step size\n",
    "        self.input_size_0 = original_step_size[0]\n",
    "        self.input_size_1 = original_step_size[1]\n",
    "\n",
    "        # a list that has the number of pooling layers\n",
    "        number_of_blocks = len(pool_list)\n",
    "\n",
    "        # Adds an initial Conv_block, identity block and max pool\n",
    "        blocks.append(conv_block(t_size=conv_size, n_step=original_step_size))\n",
    "        blocks.append(identity_block(\n",
    "            t_size=conv_size, n_step=original_step_size))\n",
    "        blocks.append(nn.MaxPool2d(pool_list[0], stride=pool_list[0]))\n",
    "\n",
    "        # adds additional layers based on the number of blocks\n",
    "        for i in range(1, number_of_blocks):\n",
    "            original_step_size = [\n",
    "                original_step_size[0]//pool_list[i-1], original_step_size[1]//pool_list[i-1]]\n",
    "            blocks.append(conv_block(t_size=conv_size,\n",
    "                          n_step=original_step_size))\n",
    "            blocks.append(identity_block(\n",
    "                t_size=conv_size, n_step=original_step_size))\n",
    "            blocks.append(nn.MaxPool2d(pool_list[i], stride=pool_list[i]))\n",
    "\n",
    "        # defines the convolutional embedding blocks\n",
    "        self.block_layer = nn.ModuleList(blocks)\n",
    "\n",
    "        self.layers = len(blocks)\n",
    "\n",
    "        original_step_size = [original_step_size[0] //\n",
    "                              pool_list[-1], original_step_size[1]//pool_list[-1]]\n",
    "\n",
    "        input_size = original_step_size[0]*original_step_size[1]\n",
    "\n",
    "        # defines the initial layer\n",
    "        self.cov2d = nn.Conv2d(1, conv_size, 3, stride=1,\n",
    "                               padding=1, padding_mode='zeros')\n",
    "\n",
    "        # defines the conv layer at the end of the conv block\n",
    "        self.cov2d_1 = nn.Conv2d(\n",
    "            conv_size, 1, 3, stride=1, padding=1, padding_mode='zeros')\n",
    "\n",
    "        self.relu_1 = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        # Layer that takes the ouput of the conv block and reduces its dimensionsl to 20\n",
    "        self.before = nn.Linear(input_size, 20)\n",
    "\n",
    "        # layer that takes the 20 parameter latent space and learns the embedding of the affine transformation\n",
    "        self.dense = nn.Linear(20, embedding_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        out = x.view(-1, 1, self.input_size_0, self.input_size_1)\n",
    "        out = self.cov2d(out)\n",
    "        for i in range(self.layers):\n",
    "            out = self.block_layer[i](out)\n",
    "        out = self.cov2d_1(out)\n",
    "\n",
    "        # flattens the conv layers so it is 1D\n",
    "        out = torch.flatten(out, start_dim=1)\n",
    "\n",
    "        # Embedding that goes to the classification layer\n",
    "        kout = self.before(out)\n",
    "\n",
    "        # Fully connected layer that helps learn the affine transformation\n",
    "        out = self.dense(kout)\n",
    "        out = self.tanh(out)\n",
    "\n",
    "        theta = out.view(-1, 2, 3)\n",
    "\n",
    "        # learns the grid of the affine tranformation\n",
    "        grid = F.affine_grid(theta.to(device), x.size()).to(device)\n",
    "\n",
    "        # applies the affine transformation to the image\n",
    "        output = F.grid_sample(x, grid)\n",
    "\n",
    "        return output, kout, theta\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "bRPxUQed6cbu",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VyQJ9qQsxe7z",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, original_step_size, up_list, embedding_size, conv_size):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        # Defines the size of the input\n",
    "        self.input_size_0 = original_step_size[0]\n",
    "        self.input_size_1 = original_step_size[1]\n",
    "\n",
    "        # dense layer used for the probability of beloning to each class\n",
    "        self.dense = nn.Linear(4, original_step_size[0]*original_step_size[1])\n",
    "\n",
    "        self.cov2d = nn.Conv2d(1, conv_size, 3, stride=1,\n",
    "                               padding=1, padding_mode='zeros')\n",
    "        self.cov2d_1 = nn.Conv2d(\n",
    "            conv_size, 1, 3, stride=1, padding=1, padding_mode='zeros')\n",
    "\n",
    "        # list where the blocks are saved\n",
    "        blocks = []\n",
    "\n",
    "        # number of blocks in the model\n",
    "        number_of_blocks = len(up_list)\n",
    "\n",
    "        # adds the blocks to the model\n",
    "        blocks.append(conv_block(t_size=conv_size, n_step=original_step_size))\n",
    "        blocks.append(identity_block(\n",
    "            t_size=conv_size, n_step=original_step_size))\n",
    "\n",
    "        for i in range(number_of_blocks):\n",
    "            # adds an upsampling layer to compensate for pooling\n",
    "            blocks.append(nn.Upsample(\n",
    "                scale_factor=up_list[i], mode='bilinear', align_corners=True))\n",
    "            original_step_size = [original_step_size[0] *\n",
    "                                  up_list[i], original_step_size[1]*up_list[i]]\n",
    "            blocks.append(conv_block(t_size=conv_size,\n",
    "                          n_step=original_step_size))\n",
    "            blocks.append(identity_block(\n",
    "                t_size=conv_size, n_step=original_step_size))\n",
    "\n",
    "        self.block_layer = nn.ModuleList(blocks)\n",
    "        self.layers = len(blocks)\n",
    "        self.output_size_0 = original_step_size[0]\n",
    "        self.output_size_1 = original_step_size[1]\n",
    "        self.relu_1 = nn.ReLU()\n",
    "        self.norm = nn.LayerNorm(4)\n",
    "\n",
    "        # used to convert probability into classification\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "        # prediction layer for classification\n",
    "        self.for_k = nn.Linear(20, 4)\n",
    "\n",
    "        # number of outputs for the classification layer\n",
    "        self.num_k_sparse = 1\n",
    "\n",
    "    def ktop(self, x):\n",
    "        # This conduct the classification based on the encoded value\n",
    "        kout = self.for_k(x)\n",
    "        kout = self.norm(kout)\n",
    "        kout = self.softmax(kout)\n",
    "        k_no = kout.clone()\n",
    "        k = self.num_k_sparse\n",
    "        with torch.no_grad():\n",
    "            if k < kout.shape[1]:\n",
    "                for raw in k_no:\n",
    "                    # computes the k-top layer\n",
    "                    indices = torch.topk(raw, k)[1].to(device)\n",
    "\n",
    "                    # creates a one-hot encoded vector\n",
    "                    mask = torch.ones(raw.shape, dtype=bool).to(device)\n",
    "                    mask[indices] = False\n",
    "                    raw[mask] = 0\n",
    "                    raw[~mask] = 1\n",
    "        return k_no\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Does the classification\n",
    "        k_out = self.ktop(x)\n",
    "\n",
    "        # uses the classification to the decoder\n",
    "        out = self.dense(k_out)\n",
    "\n",
    "        # reshapes the tensor to be an image of the size of the original image\n",
    "        out = out.view(-1, 1, self.input_size_0, self.input_size_1)\n",
    "\n",
    "        # computes the decoder\n",
    "        out = self.cov2d(out)\n",
    "        for i in range(self.layers):\n",
    "            out = self.block_layer[i](out)\n",
    "        out = self.cov2d_1(out)\n",
    "        out = self.relu_1(out)\n",
    "\n",
    "        return out, k_out\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Builds the autoencoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H06vwqpPxe7z",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class Joint(nn.Module):\n",
    "  # Module that combines the encoder and the decoder\n",
    "\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Joint, self).__init__()\n",
    "\n",
    "        # encoder and decoder\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # gets the result from the encoder\n",
    "        predicted, kout, theta = self.encoder(x)\n",
    "\n",
    "        # Builds the theta matrix\n",
    "        # We use an identity to ensure there is no translation\n",
    "        identity = torch.tensor([0, 0, 1], dtype=torch.float).reshape(\n",
    "            1, 1, 3).repeat(x.shape[0], 1, 1).to(device)\n",
    "        new_theta = torch.cat((theta, identity), axis=1).to(device)\n",
    "\n",
    "        # Computes the inverse of the affine transoformation\n",
    "        inver_theta = torch.linalg.inv(new_theta)[:, 0:2].to(device)\n",
    "\n",
    "        # Calculates the grid for inverse affine\n",
    "        grid = F.affine_grid(inver_theta.to(device), x.size()).to(device)\n",
    "\n",
    "        # Computes the decoder\n",
    "        predicted_base, k_out = self.decoder(kout)\n",
    "\n",
    "        # applies the inverse affine transformation to the decoded base\n",
    "        predicted_input = F.grid_sample(predicted_base, grid)\n",
    "\n",
    "        return predicted, predicted_base, predicted_input, k_out, theta\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Sets optional parameters for autoencoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_lIFzkPoxe7z",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Size of the original image\n",
    "en_original_step_size = [48, 48]\n",
    "\n",
    "# Defines the pooling layer\n",
    "pool_list = [2, 2, 2]\n",
    "\n",
    "# defines the size of the tensor\n",
    "de_original_step_size = [4, 4]\n",
    "\n",
    "# Defines how the upsampling should be conducted\n",
    "up_list = [2, 2, 3]\n",
    "\n",
    "# Sets the size of the embedding, this is the number of parameters in the affine\n",
    "embedding_size = 6\n",
    "\n",
    "# Sets the number of neurons in each layer\n",
    "conv_size = 128\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "LpPxYC3d-2xp",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Checks operations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KjixlRluxe70",
    "outputId": "2c5df336-8a9a-4b0a-bdcf-5fe94ffa7b1f",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# checks if cuda is available\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "# Assigns device to cuda\n",
    "device = torch.device('cuda')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Aj-kWD7dAf86",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Instantiates the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kBqkK_6qxe70",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# instantiates the encoder\n",
    "encoder = Encoder(original_step_size=en_original_step_size,\n",
    "                  pool_list=pool_list,\n",
    "                  embedding_size=embedding_size,\n",
    "                  conv_size=conv_size).to(device)\n",
    "\n",
    "# instantiates the decoder\n",
    "decoder = Decoder(original_step_size=de_original_step_size,\n",
    "                  up_list=up_list,\n",
    "                  embedding_size=embedding_size,\n",
    "                  conv_size=conv_size).to(device)\n",
    "\n",
    "# combines the two models\n",
    "join = Joint(encoder, decoder).to(device)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "RrNEt8A1Av7s",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Instantiate the optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SGrcC1vFxe70",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(join.parameters(), lr=3e-5)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Loads pretrained weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-D-ngUHDxe70",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# if you would like to train this model from scratch set equal to false.\n",
    "# Note this model takes many hours to train\n",
    "pretrained = True\n",
    "\n",
    "if pretrained:\n",
    "    # load the trained weights\n",
    "    path_checkpoint = \"./11.12_unsupervised_learn_label_epoch_17957_coef_0_trainloss_0.0008.pkl\"\n",
    "    checkpoint = torch.load(path_checkpoint, map_location=torch.device('cpu'))\n",
    "\n",
    "    join.load_state_dict(checkpoint['net'])\n",
    "    encoder.load_state_dict(checkpoint['encoder'])\n",
    "    decoder.load_state_dict(checkpoint['decoder'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    start_epoch = checkpoint['epoch']\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "LBx-rQ8hBGro",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Loss Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V9-_12ivxe70",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def loss_function(join,\n",
    "                  train_iterator,\n",
    "                  optimizer,\n",
    "                  ln_parm=1,\n",
    "                  beta=None):\n",
    "\n",
    "    # weight_decay = coef\n",
    "    # weight_decay_1 = coef1\n",
    "\n",
    "    # set the train mode\n",
    "    join.train()\n",
    "\n",
    "    # loss of the epoch\n",
    "    train_loss = 0\n",
    "\n",
    "    # loop for calculating loss\n",
    "    for x in tqdm(train_iterator, leave=True, total=len(train_iterator)):\n",
    "\n",
    "        x = x.to(device, dtype=torch.float)\n",
    "\n",
    "        # update the gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # computes the forward pass of the model\n",
    "        predicted_x, predicted_base, predicted_input, kout, theta = join(x)\n",
    "\n",
    "        # combines the loss from the affine and inverse affine transform\n",
    "        loss = F.mse_loss(predicted_base.squeeze(), predicted_x.squeeze(), reduction='mean')\\\n",
    "            + F.mse_loss(predicted_input.squeeze(),\n",
    "                         x.squeeze(), reduction='mean')\n",
    "\n",
    "        # backward pass\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # computes the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "    return train_loss\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "-sgaRX8EBur2",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Model Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JZzaf6qexe70",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def Train(join, encoder, decoder, train_iterator, optimizer,\n",
    "          epochs, coef=0, coef_1=0, ln_parm=1, beta=None, epoch_=None):\n",
    "\n",
    "    # sets the number of epochs to train\n",
    "    N_EPOCHS = epochs\n",
    "\n",
    "    best_train_loss = float('inf')\n",
    "\n",
    "    if epoch_ == None:\n",
    "        start_epoch = 0\n",
    "    else:\n",
    "        start_epoch = epoch_+1\n",
    "\n",
    "    # loops around the data set for each epoch\n",
    "    for epoch in range(start_epoch, N_EPOCHS):\n",
    "\n",
    "        # computes the loss\n",
    "        train = loss_function(join, train_iterator,\n",
    "                              optimizer, ln_parm, beta)\n",
    "\n",
    "        # updates and prints the loss\n",
    "        train_loss = train\n",
    "        train_loss /= len(train_iterator)\n",
    "        print(f'Epoch {epoch}, Train Loss: {train_loss:.4f}')\n",
    "        print('.............................')\n",
    "\n",
    "        # If the model shows improvement after 50 epochs will save\n",
    "        if best_train_loss > train_loss:\n",
    "            best_train_loss = train_loss\n",
    "            patience_counter = 1\n",
    "            checkpoint = {\n",
    "                \"net\": join.state_dict(),\n",
    "                \"encoder\": encoder.state_dict(),\n",
    "                \"decoder\": decoder.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                \"epoch\": epoch,\n",
    "            }\n",
    "            if epoch >= 50:\n",
    "                torch.save(\n",
    "                    checkpoint, f'/content/drive/MyDrive/cycle_AE_model/11.12_unsupervised_learn_label_epoch:{epoch}_coef:{coef}_trainloss:{train_loss:.4f}.pkl')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "E2Ez-cMgCPWx",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Data loader\n",
    "\n",
    "Defines an iterator that serves as the data loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "leIGbHF5xe70",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "train_iterator = torch.utils.data.DataLoader(\n",
    "    input_set, batch_size=300, shuffle=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "da4PsSkrCalq",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Trains the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MTc9_MYoxe70",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "if not pretrained:\n",
    "    # unsupervised learn label\n",
    "    Train(join, encoder, decoder, train_iterator, optimizer, 50000, epoch_=0)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "3Nv1qu_-CjMd",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Validation\n",
    "\n",
    "### Generates a small dataset for\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xOhh6qPhxe73",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# generate the test data set for validation\n",
    "angle = torch.linspace(0, 7*np.pi, 25)\n",
    "\n",
    "input_data_set_1 = []\n",
    "input_data_set_2 = []\n",
    "input_data_set_3 = []\n",
    "input_data_set_4 = []\n",
    "\n",
    "for q in angle:\n",
    "    theta = torch.tensor([\n",
    "        [torch.cos(q), torch.sin(q), 0],\n",
    "        [-torch.sin(q), torch.cos(q), 0]\n",
    "    ], dtype=torch.float)\n",
    "    grid = F.affine_grid(theta.unsqueeze(0), card_edge_1.size())\n",
    "    output = F.grid_sample(card_edge_1, grid)\n",
    "    input_data_set_1.append(output)\n",
    "\n",
    "    output = F.grid_sample(card_edge_2, grid)\n",
    "    input_data_set_2.append(output)\n",
    "\n",
    "    output = F.grid_sample(card_edge_3, grid)\n",
    "    input_data_set_3.append(output)\n",
    "\n",
    "    output = F.grid_sample(card_edge_4, grid)\n",
    "    input_data_set_4.append(output)\n",
    "\n",
    "input_set_1 = torch.stack(input_data_set_1).squeeze(1)\n",
    "input_set_2 = torch.stack(input_data_set_2).squeeze(1)\n",
    "input_set_3 = torch.stack(input_data_set_3).squeeze(1)\n",
    "input_set_4 = torch.stack(input_data_set_4).squeeze(1)\n",
    "input_set_small = torch.cat(\n",
    "    (input_set_1, input_set_2, input_set_3, input_set_4), axis=0)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "OTLbu0YNDTNA",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Builds a validation iterator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t5wp3qyGxe73",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "train_iterator = torch.utils.data.DataLoader(\n",
    "    input_set_small, batch_size=100, shuffle=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "FIGtuCR0DWZe",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Random sample visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ywERve9Vxe73",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sample = next(iter(train_iterator))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "vfQlibj2De5f",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PrrJbpk8xe73",
    "outputId": "ed0710f7-29a6-4847-93e4-7c12e1c78e00",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "out, base, inp, kout, theta = join(sample.to(device, dtype=torch.float))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "VGkr2MlRDmiV",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Visualize the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "fvmZkiI8xe73",
    "outputId": "3145b36f-2514-471d-d3bf-44e4ce417d6f",
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "label_ = [card_edge_1, card_edge_2, card_edge_3, card_edge_4]\n",
    "\n",
    "# Visualize the result\n",
    "fig, ax = plt.subplots(6, 4, figsize=(20, 30))\n",
    "for i in range(6):\n",
    "    j = np.random.randint(0, 100)\n",
    "    ax[i][0].title.set_text('input image')\n",
    "    ax[i][1].title.set_text('affine transform')\n",
    "    ax[i][2].title.set_text('generated basis')\n",
    "    ax[i][3].title.set_text(\n",
    "        f'Rotation {np.degrees(np.arccos(theta[j][0,0].detach().cpu()))}')\n",
    "    ax[i][0].imshow(sample[j].squeeze())\n",
    "\n",
    "    ax[i][1].imshow(out[j].squeeze().detach().cpu())\n",
    "\n",
    "    # ax[2].imshow((card_small.squeeze()-out[i].squeeze().detach().cpu())**2)\n",
    "    # num = torch.argmax(prob[i])\n",
    "    ax[i][2].imshow(base[j].squeeze().detach().cpu())\n",
    "\n",
    "    ax[i][3].imshow(inp[j].squeeze().detach().cpu())\n",
    "\n",
    "    # ax[3].imshow((label_[num].squeeze()-out[i].squeeze().detach().cpu())**2)\n",
    "    print(kout[j])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rmwFiBWamGVb"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "28d6d9e31a694f2aba28d42b1019d9365f56410d563150feaee59905aa4508a9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}