{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hysteresis Loops Fitting Speed Measurements Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from src.m3_learning.optimizers.AdaHessian import AdaHessian\n",
    "from src.m3_learning.util.preprocessing import global_scaler\n",
    "from src.m3_learning.be.processing import loop_lsqf, loop_fitting_function_tf, loop_fitting_function_torch\n",
    "from src.m3_learning.optimizers.TRPCGOptimizerv2 import TRPCGOptimizerv2\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.layers import (Dense, Conv1D, Flatten, MaxPooling1D, Activation)\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loads data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets path to file\n",
    "path = r\"./\"\n",
    "\n",
    "# Opens the data file2\n",
    "h5_f = h5py.File(path + \"data_file.h5\", \"r+\")\n",
    "\n",
    "# number of pixels in the image\n",
    "num_pix = h5_f[\"Measurement_000\"].attrs[\"num_pix\"]\n",
    "\n",
    "num_pix_1d = int(np.sqrt(num_pix))\n",
    "\n",
    "# number of DC voltage steps\n",
    "voltage_steps = h5_f[\"Measurement_000\"].attrs[\"num_udvs_steps\"]\n",
    "\n",
    "proj_nd_shifted = loop_lsqf(h5_f)\n",
    "proj_nd_shifted_transposed = np.transpose(proj_nd_shifted,(1,0,2,3))\n",
    "\n",
    "# getting parameters for the hysteresis loops\n",
    "params = np.array(h5_f['params_hysteresis'][:])\n",
    "params_names = ['a_0', 'a_1', 'a_2', 'a_3', 'a_4', 'b_0', 'b_1', 'b_2', 'b_3']\n",
    "\n",
    "# voltage vector\n",
    "V = np.swapaxes(np.atleast_2d(h5_f['Measurement_000']['Channel_000']['UDVS'][::2][:, 1][24:120]), 0, 1).astype(np.float64)\n",
    "\n",
    "# to set up a type of loop_fitting function to use. Possible options: ['9 parameters', '13 parameters']\n",
    "func_type = '9 parameters'\n",
    "\n",
    "# retrieve results\n",
    "real_loops = np.array(h5_f['real_loops_hysteresis'][:])\n",
    "unscaled_param_trust = np.array(h5_f['predictions_hysteresis_trustregcg'][:])\n",
    "unscaled_param_adam = np.array(h5_f['predictions_hysteresis_adam'][:])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_loops_scaler = global_scaler()\n",
    "real_scaled_loops = real_loops_scaler.fit_transform(real_loops).astype(np.float64)\n",
    "\n",
    "real_parms_scaler = StandardScaler()\n",
    "real_parms_scaled = real_parms_scaler.fit_transform(params)\n",
    "\n",
    "# getting mean and std of parameters\n",
    "params_mean = real_parms_scaler.mean_\n",
    "params_std = np.sqrt(real_parms_scaler.var_)\n",
    "\n",
    "data_mean = real_loops_scaler.mean.astype(np.float64)\n",
    "data_std = real_loops_scaler.std.astype(np.float64)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking (Adam and AdaHessian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_seed = np.arange(1, 11, 1)\n",
    "optimizers = [torch.optim.Adam, AdaHessian]\n",
    "optimizers_name = [\"ADAM\", \"ADAHESSIAN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, optim in enumerate(optimizers):\n",
    "  with open(f'{optimizers_name[k]}_Hysteresis_speed.txt', 'w') as file:\n",
    "    for seed in manual_seed:\n",
    "      file.write(f'MANUAL SEED: {seed}\\n')\n",
    "      \n",
    "      for n in range(6, 11):\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.cuda.empty_cache() \n",
    "        \n",
    "        class Pie_Model(nn.Module):\n",
    "          def __init__(self):\n",
    "              super().__init__()\n",
    "\n",
    "              # Input block of 1d convolution\n",
    "              self.hidden_x1 = nn.Sequential(\n",
    "                  nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3, padding=1),\n",
    "                  nn.LeakyReLU(),\n",
    "                  nn.MaxPool1d(kernel_size=2, padding=1),\n",
    "                  nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, padding=1),\n",
    "                  nn.LeakyReLU(),\n",
    "                  nn.MaxPool1d(kernel_size=2, padding=1),\n",
    "                  nn.Conv1d(in_channels=32, out_channels=16, kernel_size=3, padding=1),\n",
    "                  nn.LeakyReLU(),\n",
    "                  nn.MaxPool1d(kernel_size=2, padding=1),\n",
    "                  \n",
    "                  nn.Conv1d(in_channels=16, out_channels=8, kernel_size=3, padding=1),\n",
    "                  nn.LeakyReLU(),\n",
    "                  nn.MaxPool1d(kernel_size=2, padding=1),\n",
    "                  nn.Conv1d(in_channels=8, out_channels=8, kernel_size=3, padding=1),\n",
    "                  nn.LeakyReLU(),\n",
    "                  nn.MaxPool1d(kernel_size=2, padding=1),\n",
    "                  nn.Conv1d(in_channels=8, out_channels=8, kernel_size=3, padding=1),\n",
    "                  nn.LeakyReLU(),\n",
    "                  nn.MaxPool1d(kernel_size=2, padding=1),\n",
    "                  nn.Conv1d(in_channels=8, out_channels=8, kernel_size=3, padding=1),\n",
    "                  nn.LeakyReLU(),\n",
    "                  nn.MaxPool1d(kernel_size=2, padding=1),\n",
    "              )\n",
    "\n",
    "              # Flatten layer\n",
    "              self.flatten_layer = nn.Flatten()\n",
    "              \n",
    "              # Final embedding block - Output 9 values - linear\n",
    "              self.hidden_embedding = nn.Sequential(\n",
    "                  nn.Linear(16, 9),\n",
    "                  nn.LeakyReLU(),\n",
    "              )\n",
    "\n",
    "          def forward(self, x, n=-1):\n",
    "            x = torch.unsqueeze(x, 1)\n",
    "            x = self.hidden_x1(x)\n",
    "            encoded = self.flatten_layer(x)\n",
    "            embedding = self.hidden_embedding(encoded)\n",
    "            unscaled_param = embedding*torch.tensor(params_std).cuda() \\\n",
    "                                    + torch.tensor(params_mean).cuda()\n",
    "            scaled_loops = (loop_fitting_function_torch(func_type, V, unscaled_param) - torch.tensor(data_mean).cuda()) / torch.tensor(data_std).cuda()\n",
    "            return scaled_loops\n",
    "\n",
    "        \n",
    "        model = Pie_Model().cuda().double()\n",
    "\n",
    "        loss_func = torch.nn.MSELoss()\n",
    "        batch_size = 2**n\n",
    "        optimizer = AdaHessian(model.parameters(), lr=0.2) #0.1\n",
    "\n",
    "        train_dataloader = DataLoader(real_scaled_loops, batch_size=batch_size)\n",
    "\n",
    "        epochs = 1000000\n",
    "\n",
    "        file.write(f'Traning with batch size = {batch_size}\\n')\n",
    "        start_train = time.time()\n",
    "            \n",
    "        for epoch in range(epochs):\n",
    "          start_time = time.time()\n",
    "\n",
    "          train_loss = 0.\n",
    "          total_num = 0\n",
    "\n",
    "          model.train()\n",
    "\n",
    "          for train_batch in train_dataloader:\n",
    "              \n",
    "            pred = model(train_batch.double().cuda())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss = loss_func(train_batch.double().cuda(), pred)\n",
    "            loss.backward(create_graph=True)\n",
    "            train_loss += loss.item() * pred.shape[0]\n",
    "            total_num += pred.shape[0]\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "          train_loss /= total_num\n",
    "          torch.save(model, f'./Trained Models/Loops/model_{optimizers_name[k]}_bs_{batch_size}_seed_{seed}.pt')\n",
    "          torch.save(model.state_dict(), f'./Trained Models/Loops/model_{optimizers_name[k]}_bs_{batch_size}_seed_{seed}.pth')\n",
    "\n",
    "          if (time.time() - start_train) > 300:\n",
    "            file.write('Training time: ' + str(time.time() - start_train) + ' seconds\\n')\n",
    "            file.write('Number of epochs: ' + str(epoch) + '\\n')\n",
    "            break\n",
    "          \n",
    "        class Pie_Model(nn.Module):\n",
    "          def __init__(self):\n",
    "              super().__init__()\n",
    "\n",
    "              # Input block of 1d convolution\n",
    "              self.hidden_x1 = nn.Sequential(\n",
    "                  nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3, padding=1),\n",
    "                  nn.LeakyReLU(),\n",
    "                  nn.MaxPool1d(kernel_size=2, padding=1),\n",
    "                  nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, padding=1),\n",
    "                  nn.LeakyReLU(),\n",
    "                  nn.MaxPool1d(kernel_size=2, padding=1),\n",
    "                  nn.Conv1d(in_channels=32, out_channels=16, kernel_size=3, padding=1),\n",
    "                  nn.LeakyReLU(),\n",
    "                  nn.MaxPool1d(kernel_size=2, padding=1),\n",
    "                  \n",
    "                  nn.Conv1d(in_channels=16, out_channels=8, kernel_size=3, padding=1),\n",
    "                  nn.LeakyReLU(),\n",
    "                  nn.MaxPool1d(kernel_size=2, padding=1),\n",
    "                  nn.Conv1d(in_channels=8, out_channels=8, kernel_size=3, padding=1),\n",
    "                  nn.LeakyReLU(),\n",
    "                  nn.MaxPool1d(kernel_size=2, padding=1),\n",
    "                  nn.Conv1d(in_channels=8, out_channels=8, kernel_size=3, padding=1),\n",
    "                  nn.LeakyReLU(),\n",
    "                  nn.MaxPool1d(kernel_size=2, padding=1),\n",
    "                  nn.Conv1d(in_channels=8, out_channels=8, kernel_size=3, padding=1),\n",
    "                  nn.LeakyReLU(),\n",
    "                  nn.MaxPool1d(kernel_size=2, padding=1),\n",
    "              )\n",
    "\n",
    "              # Flatten layer\n",
    "              self.flatten_layer = nn.Flatten()\n",
    "              \n",
    "              # Final embedding block - Output 9 values - linear\n",
    "              self.hidden_embedding = nn.Sequential(\n",
    "                  nn.Linear(16, 9),\n",
    "                  nn.LeakyReLU(),\n",
    "              )\n",
    "\n",
    "          def forward(self, x, n=-1):\n",
    "            x = torch.unsqueeze(x, 1)\n",
    "            x = self.hidden_x1(x)\n",
    "            encoded = self.flatten_layer(x)\n",
    "            embedding = self.hidden_embedding(encoded)\n",
    "            unscaled_param = embedding*torch.tensor(params_std).cuda() \\\n",
    "                                    + torch.tensor(params_mean).cuda()\n",
    "            return unscaled_param\n",
    "        \n",
    "        model_parameters = Pie_Model().cuda().double()\n",
    "        model_parameters = torch.load(f'./Trained Models/Loops/model_{optimizers_name[k]}_bs_{batch_size}_seed_{seed}.pt')\n",
    "\n",
    "        # prediction of parameters\n",
    "        batch_size = 100000\n",
    "        train_dataloader = DataLoader(real_scaled_loops, batch_size=batch_size)\n",
    "\n",
    "        num_elements = len(train_dataloader.dataset)\n",
    "        num_batches = len(train_dataloader)\n",
    "        test_pred_params = torch.zeros_like(torch.tensor(params))\n",
    "\n",
    "        start_time_inference = time.time()\n",
    "\n",
    "        for i, train_batch in enumerate(train_dataloader):\n",
    "          start = i*batch_size\n",
    "          end = start + batch_size\n",
    "\n",
    "          if i == num_batches - 1:\n",
    "            end = num_elements\n",
    "\n",
    "          pred_batch = model_parameters(train_batch.double().cuda())\n",
    "          test_pred_params[start:end] = pred_batch.cpu().detach()\n",
    "\n",
    "          del pred_batch\n",
    "          del train_batch\n",
    "          torch.cuda.empty_cache()\n",
    "        \n",
    "        file.write(f'{optimizers_name[k]} Inference time: ' + str((time.time() - start_time_inference) / real_scaled_loops.shape[0]) + ' seconds\\n')\n",
    "\n",
    "        scaled_loops_adahessian = (loop_fitting_function_torch(func_type, V, test_pred_params.cuda()) - torch.tensor(data_mean).cuda()) / torch.tensor(data_std).cuda()\n",
    "        scaled_params_adahessian = (test_pred_params.cuda() - torch.tensor(params_mean).cuda()) / torch.tensor(params_std).cuda()\n",
    "\n",
    "        mse_loops_adahessian = mean_squared_error(scaled_loops_adahessian.cpu().detach(), torch.tensor(real_scaled_loops))\n",
    "        mse_params_adahessian = mean_squared_error(scaled_params_adahessian.cpu().detach(), torch.tensor(real_parms_scaled))\n",
    "\n",
    "        file.write(f'MSE of hysteresis loops with {optimizers_name[k]}: ' + str(np.mean(mse_loops_adahessian)) + '\\n')\n",
    "        file.write('Params Predictions MSE (scaled): ' + str(np.mean(mse_params_adahessian)) + '\\n')\n",
    "        file.write('-----------------------------\\n')\n",
    "        file.write('\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking (Trust-Region CG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTIVATION = tf.nn.leaky_relu\n",
    "\n",
    "def Conv1D_Block(X, time_step, kernel_size):\n",
    "    x = Conv1D(time_step, kernel_size, padding='same')(X)\n",
    "    x = Activation(ACTIVATION)(x)      \n",
    "    return x\n",
    "\n",
    "def Conv1D_Pie(kernel_size = 3, n_step = 96):\n",
    "    X_input = Input(shape=(n_step, 1))\n",
    "    x = X_input\n",
    "    \n",
    "    x = Conv1D_Block(x, 16, kernel_size)\n",
    "    x = MaxPooling1D(2, padding='same')(x)\n",
    "    x = Conv1D_Block(x, 32, kernel_size)\n",
    "    x = MaxPooling1D(2, padding='same')(x)\n",
    "    x = Conv1D_Block(x, 16, kernel_size)\n",
    "    x = MaxPooling1D(2, padding='same')(x)\n",
    "    \n",
    "    time_step = 8 \n",
    "    \n",
    "    x = Conv1D_Block(x, time_step, kernel_size)\n",
    "    x = MaxPooling1D(2, padding='same')(x)\n",
    "    x = Conv1D_Block(x, time_step, kernel_size)\n",
    "    x = MaxPooling1D(2, padding='same')(x)\n",
    "    x = Conv1D_Block(x, time_step, kernel_size)\n",
    "    x = MaxPooling1D(2, padding='same')(x)\n",
    "    x = Conv1D_Block(x, time_step, kernel_size)\n",
    "    x = MaxPooling1D(2, padding='same')(x)\n",
    "    \n",
    "    encoded = Flatten()(x)\n",
    "    \n",
    "    embedding = Dense(9, activation='linear')(encoded)\n",
    "    embedding = tf.cast(embedding, dtype='float64')\n",
    "    \n",
    "    unscaled_param = tf.add(tf.multiply(embedding, tf.convert_to_tensor(params_std)),\\\n",
    "                            tf.convert_to_tensor(params_mean))\n",
    "    scaled_loops = tf.divide(tf.subtract(loop_fitting_function_tf(func_type, V, unscaled_param), \\\n",
    "                            tf.convert_to_tensor(data_mean)), tf.convert_to_tensor(data_std))\n",
    "\n",
    "    model = Model(X_input, scaled_loops, name = 'Convolutional_1D')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('TRUST-REGION-CG_Hysteresis_speed.txt', 'w') as file:    \n",
    "    for seed in manual_seed:\n",
    "        file.write(f'MANUAL SEED: {seed}\\n')\n",
    "        tf.random.set_seed(seed)  \n",
    "        \n",
    "        for n in range(6, 11):\n",
    "            batch_size = 2**n\n",
    "            tf.keras.backend.set_floatx('float64')\n",
    "            model = Conv1D_Pie()\n",
    "            model.compile(optimizer=Adam(0.00001), loss='mse')\n",
    "\n",
    "            FVAL = []\n",
    "            earlyPredictor = tf.keras.Model(model.inputs,model.layers[23].output)\n",
    "            embedding = real_parms_scaled\n",
    "            unscaled_param = tf.add(tf.multiply(embedding, tf.convert_to_tensor(params_std)),\\\n",
    "                                tf.convert_to_tensor(params_mean))\n",
    "            scaled_loops_ = tf.divide(tf.subtract(loop_fitting_function_tf(func_type, V, unscaled_param), \\\n",
    "                                tf.convert_to_tensor(data_mean)), tf.convert_to_tensor(data_std))\n",
    "            \n",
    "            file.write(f'Traning with batch size = {batch_size}\\n')\n",
    "\n",
    "            cgopttol = 1e-7\n",
    "            c0tr = 0.2\n",
    "            c1tr = 0.25\n",
    "            c2tr = 0.75  # when to accept\n",
    "            t1tr = 0.75\n",
    "            t2tr = 2.0\n",
    "            radius_max = 5.0  # max radius\n",
    "            radius_initial = 1.0\n",
    "            radius = radius_initial\n",
    "\n",
    "            optimizer = TRPCGOptimizerv2(model,radius_initial,0)\n",
    "\n",
    "            allsamples=[i for i in range(num_pix)]\n",
    "\n",
    "            st = time.time()\n",
    "            for epoch in range(1000):\n",
    "\n",
    "                np.random.shuffle(allsamples)\n",
    "\n",
    "                BS = batch_size\n",
    "                for it in range(num_pix//BS):\n",
    "\n",
    "                    x = real_scaled_loops[allsamples[it*BS:(it+1)*BS]]\n",
    "                    y = real_scaled_loops[allsamples[it*BS:(it+1)*BS]]\n",
    "\n",
    "\n",
    "                    loss, d, rho, update, cg_iter, cg_term, loss_grad, norm_d, numerator, denominator, rad = \\\n",
    "                    optimizer.step(x,y)\n",
    "                    \n",
    "                    parm_pred = earlyPredictor.predict(real_scaled_loops)\n",
    "                    embedding = parm_pred\n",
    "                    unscaled_param = tf.add(tf.multiply(embedding, tf.convert_to_tensor(params_std)),\\\n",
    "                                        tf.convert_to_tensor(params_mean))\n",
    "                    scaled_loops_DNN = tf.divide(tf.subtract(loop_fitting_function_tf(func_type, V, unscaled_param), \\\n",
    "                                        tf.convert_to_tensor(data_mean)), tf.convert_to_tensor(data_std))\n",
    "\n",
    "                    \n",
    "                    err = tf.reduce_mean(tf.abs(scaled_loops_DNN - real_scaled_loops)).numpy()\n",
    "                    \n",
    "                    FVAL.append([loss.numpy(),err])\n",
    "                    if optimizer.radius < 1e-15:\n",
    "                        break\n",
    "\n",
    "                if(time.time() - st > 300):\n",
    "                    file.write('Training time: ' + str(time.time() - st) + ' seconds\\n')\n",
    "                    file.write('Number of epochs: ' + str(epoch) + '\\n')\n",
    "                    break\n",
    "\n",
    "            model.save(f'./Trained Models/Loops/model_bs{batch_size}')\n",
    "            unscaled_param_trust = tf.identity(unscaled_param)\n",
    "            scaled_params_trust = tf.identity(embedding)\n",
    "\n",
    "            start_time_inference = time.time()\n",
    "            earlyPredictor.predict(real_scaled_loops)\n",
    "            file.write('Trust Region Inference time: ' + str((time.time() - start_time_inference) / real_scaled_loops.shape[0]) + ' seconds\\n')\n",
    "\n",
    "            scaled_loops_DNN_trust = scaled_loops_DNN\n",
    "            real_scaled_loops_trust = real_scaled_loops\n",
    "            scaled_loops_trust = scaled_loops_\n",
    "            \n",
    "            start_time_inference = time.time()\n",
    "            earlyPredictor.predict(real_scaled_loops)\n",
    "        \n",
    "            errors = tf.reduce_mean(tf.abs(scaled_loops_DNN_trust - real_scaled_loops) + tf.abs(scaled_loops_DNN - real_scaled_loops), 1)\n",
    "\n",
    "            mse = tf.keras.losses.MeanSquaredError()\n",
    "            trust_region_error = mse(scaled_loops_DNN_trust, real_scaled_loops).numpy()\n",
    "\n",
    "            mae = tf.keras.losses.MeanAbsoluteError()\n",
    "            trust_region_mae = mae(scaled_loops_DNN_trust, real_scaled_loops).numpy()\n",
    "\n",
    "            errors = np.asarray(errors)\n",
    "            trust_region_error = np.asarray(trust_region_error)\n",
    "\n",
    "            unscaled_loops_lsqf = loop_fitting_function_tf(func_type, V, params)\n",
    "            scaled_loops_lsqf = tf.divide(tf.subtract(loop_fitting_function_tf(func_type, V, params), \\\n",
    "                                        tf.convert_to_tensor(data_mean)), tf.convert_to_tensor(data_std))\n",
    "            \n",
    "            mse_loops_trust = np.mean(np.square((scaled_loops_DNN_trust - real_scaled_loops)), 1)\n",
    "            mse_loops_lsqf = np.mean(np.square((scaled_loops_lsqf - real_scaled_loops)), 1)\n",
    "            highest_loops_trust = (-mse_loops_trust).argsort()[:]\n",
    "            highest_loops_lsqf = (-mse_loops_lsqf).argsort()[:]\n",
    "            file.write('MSE of hysteresis loops with Trust Region CG: ' + str(np.mean(mse_loops_trust)) + '\\n')\n",
    "            file.write('MSE of hysteresis loops with LSQF: ' + str(np.mean(mse_loops_lsqf)) + '\\n')\n",
    "            file.write('Params Predictions MSE (scaled): ' + str(mse(scaled_params_trust, real_parms_scaled)) + '\\n')\n",
    "            file.write('Params Predictions MSE (unscaled): ' + str(mse(unscaled_param_trust, params)) + '\\n')\n",
    "            file.write('-----------------------------\\n')\n",
    "            file.write('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapid_fitting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c58f42fd11d8ae4df132d3c425059695e86ccc63a852aa66615442730ca8b1fc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
